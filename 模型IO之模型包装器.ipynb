{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63792fa0-d61f-411c-9ee8-c6e218b21a25",
   "metadata": {},
   "source": [
    "# 使用langchain调用openai的接口\n",
    "# 使用LLM模型包装器进行调用\n",
    "# 使用聊天模型包装器进行调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f70b3dc-d60e-416b-9745-6162dce4904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f331e-b517-4494-b3fc-c26e0f6669e5",
   "metadata": {},
   "source": [
    "## 使用LLM模型包装器进行调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d92ee0-eaa4-4c6b-9f57-9d53f34dc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02e49ed-2bc0-4c3f-b5de-d234dd68c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "邓小平被认为是中国改革开放的总设计师。他在1978年提出了改革开放的战略思想，并领导了中国的改革开放进程，为中国的经济发展和社会变革做出了重大贡献。 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key = api_key, temperature=0.5)\n",
    "response = llm.invoke('谁是中国改革开放的总设计师？')\n",
    "print(response,type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db06096-024d-4ccc-a0f7-2aa95489a2a8",
   "metadata": {},
   "source": [
    "## 使用聊天模型包装器进行调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70ae29b-0501-4e45-8701-02c2d81c9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='中国改革开放的总设计师是邓小平。' <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=api_key,temperature = 0.5)\n",
    "\n",
    "response = llm.invoke(\"中国改革开放的总设计师是谁?\")\n",
    "\n",
    "print(response,type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d7d674-e8ea-4dda-a614-e87c56c32ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# client = InferenceClient(token=\"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\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个中国历史专家.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "        model_kwargs={'temperature':0},\n",
    "        huggingfacehub_api_token='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'\n",
    ")\n",
    "\n",
    "chain = prompt | hub_llm\n",
    "\n",
    "chain.invoke({\"input\":\"中国改革开放的总设计师是谁?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8680aab-ac62-4fc5-9753-81c21cae16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-Zq40bukilVJ01Ga097486d2790Ae483cA5F7A4C86b5dAaE3\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(os.getenv(\"OPENAI_API_BASE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b9e533-a20c-493b-848c-0ec7717564fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "BASE_URL = \"https://apejhvxcd.cloud.sealos.io/v1\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    openai_api_base=BASE_URL,\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"谁是中国改革开放的总设计师？\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965a452c-8576-4a8e-bc27-17cf953b96e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='邓小平被认为是中国改革开放的总设计师。'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2ecbea-ab9a-4113-9ec9-b370cd40b9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(llm(text))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mtext\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat would be a good company name for a company that makes colorful socks?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:948\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    945\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    949\u001b[0m         [prompt],\n\u001b[0;32m    950\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    951\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    952\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    953\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    954\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    955\u001b[0m     )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    958\u001b[0m )\n",
      "File \u001b[1;32mD:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:698\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    684\u001b[0m         )\n\u001b[0;32m    685\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    686\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    687\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     ]\n\u001b[1;32m--> 698\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    699\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    700\u001b[0m     )\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:562\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    561\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    563\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mD:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:549\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    541\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 549\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    550\u001b[0m                 prompts,\n\u001b[0;32m    551\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    552\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    553\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    554\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    555\u001b[0m             )\n\u001b[0;32m    556\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    557\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    558\u001b[0m         )\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mD:\\chatbot\\jupyter\\.venv\\lib\\site-packages\\langchain_community\\llms\\openai.py:466\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m()\n\u001b[0;32m    468\u001b[0m choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    469\u001b[0m update_token_usage(_keys, response, token_usage)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'dict'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import time\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "API_SECRET_KEY = \"sk-Zq40bukilVJ01Ga097486d2790Ae483cA5F7A4C86b5dAaE3\"\n",
    "BASE_URL = \"https://apejhvxcd.cloud.sealos.io\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_SECRET_KEY\n",
    "os.environ[\"OPENAI_API_BASE\"] = BASE_URL\n",
    "\n",
    "def text():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    print(llm(text))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
